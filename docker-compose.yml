version: "3.8"

services:
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME:-smolvlm}-backend
    runtime: nvidia                     # ← GPU-доступ (старый надёжный способ)
    environment:
      - DEVICE=cuda                      # ← принудительно включаем GPU
      - NVIDIA_VISIBLE_DEVICES=all       # ← обязательно при runtime: nvidia
      - VQA_MODEL_ID=${VQA_MODEL_ID:-HuggingFaceTB/SmolVLM2-256M-Video-Instruct}
      - MODEL_SIZE=${MODEL_SIZE:-256M}
      - PORT=${BACKEND_INTERNAL_PORT:-8000}
      - HF_HOME=/root/.cache/huggingface
      - MODELS_DIR=/models
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    volumes:
      - ./backend:/app/backend           # ← ЭТО ГЛАВНОЕ: твой код теперь внутри!
      - ./backend/uploads:/app/uploads
      - ./models:/models
      - huggingface_cache:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s                 # модель на GPU грузится дольше
    networks:
      - smolvlm-net
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME:-smolvlm}-frontend
    ports:
      - "${FRONTEND_PORT:-8501}:8501"
    environment:
      - BACKEND_URL=http://backend:8000
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - smolvlm-net
    restart: unless-stopped

networks:
  smolvlm-net:
    driver: bridge

volumes:
  huggingface_cache: